---
slug: first-blog-post-1
title: RL based hyper-parameters optimization algorithm (ROA) for convolutional neural network
authors:
  name: Million Dollar Team
  title: RL based hyper-parameters optimization algorithm (ROA) for convolutional neural network
  url: https://link.springer.com/article/10.1007/s12652-022-03788-y
  image_url: https://media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs12652-022-03788-y/MediaObjects/12652_2022_3788_Fig1_HTML.png?as=webp
tags: [РЛ, НГУ, МДТ, Статьи]
---

People can recognize items in their environment and objects in less than a second. Humans have been taught to recognize things since they were children. Similarly, if computers can detect or categorize objects and environments by scanning for low-level characteristics like edges and curves, they may use a sequence of convolutional layers to develop more abstract conceptions of what they see. Convolutional Neural Networks are used in neural networks to recognize and classify images (CNN). Image recognition is used in a variety of applications (Sehgal et al. 2019). Deep Neural Networks are responsible for most of the success in this area. While deep networks have enabled numerous intriguing and valuable applications, there are still several challenges to solve (Tian et al. 2020; Calabrese et al. (2020). One impediment is the lack of an analytical method for determining the appropriate design for a deep network for tackling various issues. For many articles, writers design multiple distinct network topologies before deciding on the optimal one to utilize. This causes a knowledge and effort load in determining the appropriate architecture. Manually selecting and executing these architectures might be time intensive (Duong et al. 2022).

The implementation of CNN necessitates a set of settings that are independent of the data and that the machine learning researcher must manually modify. Hyperparameters are variables that affect the network structure and CNN's trained network (Tian et al. 2020). The hyperparameter optimization challenge also includes finding a collection of hyperparameters that produces an accurate model in an acceptable amount of time. The task of identifying a suitable model of hyperparameter or the problem of optimizing a loss function across a graph-structured configuration space is known as hyperparameter optimization. It can be computationally costly to test every potential set of hyperparameter models. As a result, the demand for an automated and organized search method is growing, and hyperparameter space, in general, is expanding.

Hyperparameter optimization is used to increase the accuracy of neural networks. which has a lot of real-world applications such as signature verification and handwriting analysis, healthcare, traveling salesman problem, image compression, stock exchange prediction, computer vision, speech recognition, and natural language processing(Abiodun et al. 2018, Thanga et al. 2021).

The typical method of accomplishing hyperparameter optimization has been grid search (Chicco et al. 2017) or parameter sweep, which is an exhaustive search of a manually chosen subset of a learning algorithm's hyperparameter space. A grid search algorithm must be directed by a performance metric, which is commonly assessed by cross-validation on the training set or assessment on a held-out validation set. Figure 1 illustrates that Grid search using two hyperparameters with varying values. Each hyperparameter is assessed and compared with ten distinct values, for a total of 100 possible combinations. Blue outlines represent places with strong outcomes, while red contours represent regions with low results.

![alt text](https://media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs12652-022-03788-y/MediaObjects/12652_2022_3788_Fig1_HTML.png?as=webp)