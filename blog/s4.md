---
slug: first-blog-post-4
title: RL Algorithms
authors:
  name: Million Dollar Team
  title: RL Algorithms
  url: https://stable-baselines3.readthedocs.io/en/master/guide/algos.html
  image_url: https://stable-baselines3.readthedocs.io/en/master/_static/logo.png
tags: [РЛ, НГУ, МДТ, Статьи]
---

## Tuple observation spaces

Tuple observation spaces are not supported by any environment; however, single-level Dict spaces are (cf. Examples).

### Actions gym.spaces:

- **Box**: A N-dimensional box that contains every point in the action space.
  
- **Discrete**: A list of possible actions, where each timestep only one of the actions can be used.
  
- **MultiDiscrete**: A list of possible actions, where each timestep only one action of each discrete set can be used.
  
- **MultiBinary**: A list of possible actions, where each timestep any of the actions can be used in any combination.

> **Note**
> More algorithms (like QR-DQN or TQC) are implemented in our contrib repo.

> **Note**
> Some logging values (like `ep_rew_mean`, `ep_len_mean`) are only available when using a Monitor wrapper. See [Issue #339](link_to_issue) for more info.

> **Note**
> When using off-policy algorithms, Time Limits (aka timeouts) are handled properly (cf. [issue #284](link_to_issue)). You can revert to SB3 < 2.1.0 behavior by passing `handle_timeout_termination=False` via the `replay_buffer_kwargs` argument.

### Reproducibility

Completely reproducible results are not guaranteed across PyTorch releases or different platforms. Furthermore, results need not be reproducible between CPU and GPU executions, even when using identical seeds.

In order to make computations deterministic on your specific problem on one specific platform, you need to pass a seed argument at the creation of a model. If you pass an environment to the model using `set_env()`, then you also need to seed the environment first.

*Credit: Part of the Reproducibility section comes from [PyTorch Documentation](link_to_pytorch_docs)*.


## Comparison of Reinforcement Learning Algorithms

| Algorithm              | Type                    | Observation Space | Action Space           | Exploration Strategy | Use Cases                                   |
|------------------------|-------------------------|-------------------|------------------------|-----------------------|----------------------------------------------|
| Q-Learning             | Value-Based            | Discrete          | Discrete               | Epsilon-Greedy        | Single-Agent, Discrete Action Spaces         |
| Deep Q Network (DQN)   | Deep RL, Value-Based   | Continuous        | Discrete               | Epsilon-Greedy        | Complex Environments, High-Dimensional State Spaces |
| Policy Gradient        | Policy-Based           | Continuous        | Continuous            | Stochastic Policy     | High-Dimensional Action Spaces, Continuous Control |
| Proximal Policy Optimization (PPO) | Policy-Based | Continuous        | Continuous            | Clipped Surrogate     | Stable Training, Continuous Control          |
| Actor-Critic           | Hybrid                 | Continuous        | Continuous            | Epsilon-Greedy        | Balance of Exploration and Exploitation      |
| Trust Region Policy Optimization (TRPO) | Policy-Based | Continuous    | Continuous            | Trust Region Methods  | Stable Training, Continuous Control          |
| Soft Actor-Critic (SAC)| Soft Actor-Critic (SAC)| Continuous        | Continuous            | Entropy Regularization| Robustness, Continuous Control               |
| Twin Delayed DDPG (TD3)| Deep RL, Value-Based   | Continuous        | Continuous            | Exploration Noise     | Continuous Control, Robust to Noisy Environments |
| A3C (Asynchronous Advantage Actor-Critic) | Hybrid | Continuous    | Discrete               | Asynchronous Training| Parallel Environments, Discrete Actions      |

