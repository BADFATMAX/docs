---
sidebar_position: 6
---

# НАБЛЮДЕНИЕ, ДЕЙСТВИЕ, НАГРАДА


В качестве входных данных для агента `(observation)` берутся метрики, 
полученные в ходе обучения, и архитектуры в количестве, равном длине памяти. 
Агент принимает решение о построении новых нейросетей 
на основе пар: (предыдущая архитектура, метрики после обучения)

```mdx
obs={
'metrics': shape=[mem_len, amount_of_metrics, train_epochs],
'architectures': shape=[mem_lem, layers_amount],
}

```

```mdx
action = []
action.shape=[max_layers_amount]
```
:::tip
Действие агента `(action)` выглядит как набор чисел фиксированной длины, 
каждому из которых соответствует слой нейросети из списка возможных `(actions_set)`.
:::
---
Награды для агента
---

Награда агента (reward) - зачастую самая сложная для формализации часть, так как от этого зависит, что агент примет за хорошо, а что за плохо. Мы будем награждать агента следующим образом:

### metrics_optimization_reward

В случае, когда метрики улучшаются, агент заслуживает поощрение. Рассчитаем награду как разницу между последними значениями отслеживаемых метрик и новыми. В зависимости от знака получившегося числа, награда может превратиться в наказание за построение модели хуже, чем в последний раз.

- **Замечание 1:** Метрики первой эпохи могут представлять собой выбросы большой величины, поэтому мы будем пропускать их, производя по сути `train_epochs + 1` эпох обучения.

- **Замечание 2:** Не самые оптимальные архитектуры могут выдавать огромные показатели метрики. Для сглаживания этого момента мы будем ограничивать сверху значения эмпирически подобранным порогом (в частности, было использовано значение 100).

- **Замечание 3:** Иногда в особо печальных случаях метрики вообще могут принять значение nan или inf. Будем также заменять такие значения на `NAN_NUM = 100`.

### creation_successfull_reward

Если нейросеть получилось создать, будем награждать агента за нейросети, которые получилось корректно построить, и штрафовать в обратном случае. Для этого созданы переменные:

- `NN_CREATE_SUCCESS_REWARD = 1`
- `NN_CREATE_NOT_SUCCESS_PENALTY = -1`

### optimal_depth_reward
:::danger
Эта часть является заделом на будущее, на текущий момент не используется.
:::