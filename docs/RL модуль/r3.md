---
sidebar_position: 6
---

# RL Custom environment

:::note
Для реализации будет использоваться python с библиотеками gymnasium и stable-baselines3.
:::

## Структура среды в gymnasium

Среда, с которой взаимодействует агент, принимая наблюдение и награду, и отдавая новое действие-реакцию на изменения, наследуется от класса `gym.Env` из библиотеки gymnasium. Среда состоит из трёх обязательных методов:

### `reset`

Метод перезапуска среды перед началом работы: обнуление переменных, выделение необходимого объёма памяти и т.д.

### `render`

Метод взаимодействия с графическим интерфейсом среды. В случае отсутствия какой-либо визуализации, можно оставить пустым.

### `step`

Метод преобразования действия агента, создания нового наблюдения, вычисления наград и штрафов, поднятия флага завершения эпизода. В нём происходит основная часть логики среды.

```python

class Env(gym.Env):
  def __init__(self):
    # init all required variables
    # action and observation spaces must be here!

    self.action_space = spaces.Box(...)
    self.observation_space = spaces.MultiDiscrete(...)
    pass

  def seed(self):
     seed = 42
     return [seed]

  def step(self, action):
    # most important metrics
    # do manipulations with action, 
    # return new observation and reward
    return obs, reward, _, done, info

  def reset(self):
    # set variables to start state
    return obs

  def render(self):
    # all GUI parts should be here
    # may be empty
    pass
```

## Переменные среды

```python
  def __init__(self, opt_cls=None, crit=None, trn_ldr=None, vld_ldr=None, render_mode=None):
      '''
      Initialization of environment variables
      '''

      # here is learning params
      self.NN_PARAMS = {
          'lr': 0.001,
          'num_classes': 10, # TODO adaptive num_classes
          'train_epochs': 10,
          'last_nets_metrics_memory_len': 10,
          'layers_amount': 5,
          'amount_of_metrics': 2,
      }

      # here contains NN learning metrics
      # uses for observations
      # shape=[MEMORY_LEN, N_METRICS, EPOCHS]
      self.NN_PARAMS['metrics'] = np.zeros((
          self.NN_PARAMS['last_nets_metrics_memory_len'],
          self.NN_PARAMS['amount_of_metrics'],
          self.NN_PARAMS['train_epochs'],
      ))

      # here contains last architectures
      # uses for observations
      # shape=[MEMORY, N_LAYERS]
      self.NN_PARAMS['last_nets_architectures'] = np.zeros((
             self.NN_PARAMS['last_nets_metrics_memory_len'],
             self.NN_PARAMS['layers_amount'],
          ))

      # Variables for NN
      self.Net = self.NN()
      self.train_dataloader = trn_ldr
      self.valid_dataloader = vld_ldr
      self.optimizer_class = opt_cls
      self.optimizer = None
      self.criterion = crit
      self.device = 'cuda'

      self.nngenerator = self.nnGenerator()

      self.last_obs = None

      # Action space describes what agent will give to environment
      # shape=[ACTION_SET_SIZE, N_LAYERS]
      self.action_space = spaces.MultiDiscrete(
        [len(self.actions_set)] * self.NN_PARAMS['layers_amount'],
         seed=42)

      # Observation space describes what agent
      # will take from enviromnent as observation
      # shape=dict{
      #  METRICS, shape as NN_PARAMS['metrics'],
      #  ARCHITECTURES, shape as NN_PARAMS['layers_amount'],
      # }
      self.observation_space = spaces.Dict(
          {
          'last_nets_metrics_memory': spaces.Box(
              low=0,
              high=100,
              shape=(self.NN_PARAMS['last_nets_metrics_memory_len'],
                     self.NN_PARAMS['amount_of_metrics'],
                     self.NN_PARAMS['train_epochs'],
                     )),
          'last_nets_architectures': spaces.Box(
              low=0,
              high=len(self.actions_set),
              shape=(self.NN_PARAMS['last_nets_metrics_memory_len'],
                     self.NN_PARAMS['layers_amount'],
                     ))
          }
      )

      # some variable for collecting statistics

      self.statistics = {
        'episode_rewards': [],
        'global_rewards': [],
        'made_steps': [],
      }

      self.seed()
      assert render_mode is None or render_mode in self.metadata["render_modes"]
      self.render_mode = render_mode
```

:::info
На вход принимаются оптимайзер, функция ошибки и даталоадеры.
:::

Здесь указаны все необходимые параметры. Создается объект пустой нейросети `Net`, в который мы позже будем помещать построенную нейросеть, а также `nnGenerator`, который занимается обработкой действия агента и построением самой сети. Обязательными полями являются `action_space` и `observation_space`. Без их определения создание среды невозможно.

**Замечание:** К выбору пространства действий и наблюдений следует подходить с умом, так как алгоритмы агента из `stable-baselines3` могут не поддерживать тот формат, который вы выберете. О том, какие пространства можно выбрать для алгоритмов, можно посмотреть [тут](ссылка).

Мы будем использовать PPO ('MultiInputPolicy') — proximal policy optimization алгоритм, о котором можно прочитать в документации к библиотеке `stable-baselines3`.

## Перезагрузка среды

```python

  def reset(self, seed=None, options=None):
      '''
      Reset the env,
      Set all changed in training proccess variables to zero
      (or noise, dependse on your realization)

      seed: list, list of random seeds (depricated)
      options: list, additional options (future)
      '''
      super().reset(seed=seed)
      self.Net = self.NN()

      current_obs = {
          'last_nets_metrics_memory': np.zeros((
              self.NN_PARAMS['last_nets_metrics_memory_len'],
              self.NN_PARAMS['amount_of_metrics'],
              self.NN_PARAMS['train_epochs'],
             ) ),
          'last_nets_architectures': np.zeros((
             self.NN_PARAMS['last_nets_metrics_memory_len'],
             self.NN_PARAMS['layers_amount'],
          ) )

          }
      self.NN_PARAMS['metrics'] = np.zeros((
          self.NN_PARAMS['last_nets_metrics_memory_len'],
          self.NN_PARAMS['amount_of_metrics'],
          self.NN_PARAMS['train_epochs'],
      ))
      self.NN_PARAMS['last_nets_architectures'] = np.zeros((
             self.NN_PARAMS['last_nets_metrics_memory_len'],
             self.NN_PARAMS['layers_amount'],
          ))

      self.last_obs = current_obs
      self.episode_reward = 0
      self.current_it = 1

      self.statistics['episode_rewards'] = []
      self.statistics['made_steps'] = []

      return current_obs, {'none': None}

```

## Шаг среды

---

Основная функция среды принимает действие (action), создает нейросеть (NN), обучает нейросеть, вычисляет новое наблюдение (obs) и вознаграждение (reward).

### Входные параметры
- `action`: список, форма действия аналогична пространству действий (action_space).

### Возвращаемые значения
- `obs`: массив NumPy,
- `reward`: число с плавающей запятой,
- `done`: False, флаг завершения обучения агента, необходим, когда ваши действия достигают какого-то завершающего состояния,
- `info`: словарь, вы можете добавить дополнительную информацию.

### Алгоритм
1. Разбор действия.
2. Генерация нейросети.
3. Обновление оптимизатора, подготовка к обучению.
4. Обучение нейросети, сбор метрик.
5. Вычисление вознаграждения.
6. Сбор статистики.
7. Создание нового наблюдения.
8. Возврат obs, reward, done, info.

```python
  def step(self, action):
      '''
          Main environment function

          takes action, creates NN, train NN, calc new obs and reward

          action: list, shape of action is like action_space

      return:
      obs: np.array,
      reward: float,
      done = False, end of agent training flag, needed when your actions
      achieved some finish state
      info: dict, you may need to add some extra information, put it here

      Algorithm
      1) parse action
      2) generate NN
      3) update optimizer, prepare for training
      4) train NN, collect metrics
      5) calculate reward
      6) collect statistics
      7) create new observation
      8) return obs, reward, done, info

      '''
      reward = 0
      done = False
      info = {}
      self.nngenerator.parse_action(action, self.actions_set)
      test_b = self.get_test_batch()
      success_state, net = self.nngenerator.generateNN(n_classes=self.NN_PARAMS['num_classes'], test_batch=test_b)

      new_metrics = None
      if success_state == True: # NN created_correctly
        self.NN_PARAMS['last_nets_architectures'] = np.roll(self.NN_PARAMS['last_nets_architectures'], -1, axis=0)
        self.NN_PARAMS['last_nets_architectures'][-1] = action
        self.Net.layers = net
        display(self.Net)
        self.Net = self.Net.to(self.device)
        self.optimizer = self.optimizer_class(self.Net.parameters(), lr=self.NN_PARAMS['lr'])
        new_metrics = self.train()


      reward = self.calc_reward(success_state,
                                self.nngenerator.get_nn_len(),
                                new_metrics,
                                )


      current_obs = self.create_obs()

      self.last_obs = current_obs

      print('Reward: ', reward)
      self.episode_reward = reward
      self.current_it += 1
      return current_obs, self.episode_reward, None, done, info
```

:::danger
В этом методе происходит вся магия. Создаем нейросеть, обучаем, собираем метрики, считаем награду, фиксируем прибыль ヽ( ▀̿ Ĺ̯ ▀̿)ノ.
:::

Вспомогательными методами здесь являются `get_test_batch`, `calc_reward` и `create_obs`.
- `get_test_batch` извлекает из датасета один батч, необходимый для вычисления входной формы данных при построении нейросети.
- `create_obs` возвращает новое наблюдение, объединяя метрики и архитектуры в словарь формата, заданного в `observation_space`.
- `calc_reward` подсчитывает, какой награды (или наказания) будет удостоен агент.

```python

  def calc_reward(self, nn_created_correctly_flag, nn_len, last_train_metrics):
    '''
       calculate agent reward

       nn_created_correctly_flag: bool,
       if True - NN was built successfully,
       we can calculate other parts of reward,
       otherwise - agent takes NN_CREATE_NOT_SUCCESS_PENALTY only

       nn_len: int,
       that var needed for depth decreasing reward
       # Not used (future) #

       last_train_metrcis: np.array,
       contains last training loop metrics
       for metrics_optimization_reward

       return reward: float, sum of all reward parts

    '''

    reward = 0
    # TODO reward for decreasing nn depth
    optimal_depth_reward = 0
    # reward by metrics
    metrics_optimization_reward = 0
    # reward for successfull nn creation
    creation_successfull_reward = 0
    if nn_created_correctly_flag == True:
      # do not reward agent if creation is not succeed
      creation_successfull_reward += self.NN_CREATE_SUCCESS_REWARD

      last_metrics = self.NN_PARAMS['metrics'][-1]
      last_train_metrics = np.nan_to_num(x=last_train_metrics, nan=self.NAN_NUM)
      last_train_metrics = np.minimum(last_train_metrics,
                          np.ones(shape=(self.NN_PARAMS['amount_of_metrics'], self.NN_PARAMS['train_epochs'])) * self.NAN_NUM
                          )

      tmp_r = np.min(last_metrics,axis=1) - np.min(np.array(last_train_metrics), axis=1)


      self.NN_PARAMS['metrics'] = np.roll(self.NN_PARAMS['metrics'], -1, axis=0)
      self.NN_PARAMS['metrics'][-1] = last_train_metrics
      metrics_optimization_reward = np.sum(self.METRICS_OPTIMIZATION_FACTOR * tmp_r)

      optimal_depth_reward = (self.NN_PARAMS['layers_amount'] - nn_len)
      optimal_depth_reward *= self.DEPTH_REWARD_FACTOR

    else:
      creation_successfull_reward += self.NN_CREATE_NOT_SUCCESS_PENALTY

    reward += optimal_depth_reward
    reward += metrics_optimization_reward
    reward += creation_successfull_reward

    return reward



  def get_test_batch(self):
    '''
        return: batch: np.array, one batch
        for input_shape in NN building algorithm

    '''
    batch = None
    for  b, _ in self.train_dataloader:
      batch = b
    return batch




  def create_obs(self):
    '''
       return obs: dict, new observation
    '''

    obs = {
          'last_nets_metrics_memory': self.NN_PARAMS['metrics'],
          'last_nets_architectures': self.NN_PARAMS['last_nets_architectures']

          }
    return obs
```